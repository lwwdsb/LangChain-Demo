{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOmvQAdYswlsmvAyVfyPcYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lwwdsb/LangChain-Demo/blob/main/LangChainDemo.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkd12j1_GMbk",
        "outputId": "161e17e1-f96f-42f7-89aa-761b017406eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mFound existing installation: langchain 1.2.10\n",
            "Uninstalling langchain-1.2.10:\n",
            "  Successfully uninstalled langchain-1.2.10\n",
            "Found existing installation: langchain-core 1.2.13\n",
            "Uninstalling langchain-core-1.2.13:\n",
            "  Successfully uninstalled langchain-core-1.2.13\n",
            "Found existing installation: langchain-community 0.4.1\n",
            "Uninstalling langchain-community-0.4.1:\n",
            "  Successfully uninstalled langchain-community-0.4.1\n",
            "Found existing installation: langgraph 1.0.8\n",
            "Uninstalling langgraph-1.0.8:\n",
            "  Successfully uninstalled langgraph-1.0.8\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.2.10-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting langchain-core\n",
            "  Using cached langchain_core-1.2.13-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Downloading langchain-1.2.10-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached langchain_core-1.2.13-py3-none-any.whl (500 kB)\n",
            "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "Downloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langgraph, langchain-community, langchain\n",
            "Successfully installed langchain-1.2.10 langchain-community-0.4.1 langchain-core-1.2.13 langgraph-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-openai langchain-community faiss-cpu tiktoken\n",
        "\n",
        "!pip uninstall -y langchain langchain-core langchain-community langgraph\n",
        "!pip install -U langchain langchain-core langchain-community langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "print(\"LangChain æ­£å¸¸\")\n",
        "\n",
        "import langchain_core\n",
        "print(langchain_core.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvaZooIDGWmZ",
        "outputId": "41f1d782-54e6-4a75-90e5-23093b2d1d20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain æ­£å¸¸\n",
            "1.2.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"è¯·è¾“å…¥ä½ çš„ OpenAI API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoQEdhYhHxlN",
        "outputId": "d4316093-4d04-4e9c-c8d2-969e093a0907"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¯·è¾“å…¥ä½ çš„ OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "å‡†å¤‡ç®€æ˜“çŸ¥è¯†åº“"
      ],
      "metadata": {
        "id": "wFZChJ5aI9Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚\"),\n",
        "    Document(page_content=\"RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç”¨äºå¢å¼ºå¤§æ¨¡å‹å›ç­”çš„å‡†ç¡®æ€§ã€‚\"),\n",
        "    Document(page_content=\"Agent å¯ä»¥è°ƒç”¨å·¥å…·æ¥æ‰§è¡Œä»»åŠ¡ã€‚\")\n",
        "]\n",
        "\n",
        "print(\"æ–‡æ¡£åˆ›å»ºæˆåŠŸ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI_xMNblIrdP",
        "outputId": "268a031b-d5d0-4188-e232-e5264ee51903"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ–‡æ¡£åˆ›å»ºæˆåŠŸ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ–‡æ¡£åˆ‡åˆ†"
      ],
      "metadata": {
        "id": "21YBs30SJZcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"åˆ‡åˆ†åçš„æ–‡æ¡£æ•°é‡: {len(split_docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBMaykTAJA3Y",
        "outputId": "29f07414-9ec7-4ed5-d26d-7e0d46b03a4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "åˆ‡åˆ†åçš„æ–‡æ¡£æ•°é‡: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "å‘é‡åŒ– + å»ºç«‹å‘é‡æ•°æ®åº“"
      ],
      "metadata": {
        "id": "SN2Qjfv-J3_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#embeddings å’Œé•¿æœŸå‘é‡åº“åˆå§‹åŒ–ï¼ˆè¿è¡Œä¸€æ¬¡ï¼‰\n",
        "import time\n",
        "\n",
        "long_term_store = FAISS.from_documents(\n",
        "    [Document(page_content=\"åˆå§‹åŒ–\")],\n",
        "    embeddings\n",
        ")\n"
      ],
      "metadata": {
        "id": "nCa8hcsJJkko"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "åˆ›å»º LLM"
      ],
      "metadata": {
        "id": "_2tQY_i3KC5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "e9bEAIKnJ9Ii"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_llm(prompt_text: str) -> str:\n",
        "    result = llm.invoke(prompt_text)\n",
        "    return result.content"
      ],
      "metadata": {
        "id": "oK3Ixh7oug4d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å®šä¹‰ Tools"
      ],
      "metadata": {
        "id": "Bux17rKbLjws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "!pip install -q duckduckgo-search\n",
        "!pip install -U ddgs\n",
        "\n",
        "# å·¥å…· 1ï¼šçŸ¥è¯†åº“æ£€ç´¢\n",
        "@tool\n",
        "def search_knowledge_base(query: str) -> str:\n",
        "    \"\"\"å½“ç”¨æˆ·è¯¢é—® LangChainã€RAG æˆ– Agent æŠ€æœ¯ç»†èŠ‚æ—¶ä½¿ç”¨ã€‚\"\"\"\n",
        "    docs = retriever.invoke(query)\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "# å·¥å…· 2ï¼šé•¿æœŸè®°å¿†æ£€ç´¢\n",
        "@tool\n",
        "def search_long_term_memory(query: str) -> str:\n",
        "    \"\"\"å½“ç”¨æˆ·è¯¢é—®å†å²æ€»ç»“æˆ–ä¹‹å‰å¯¹è¯æ—¶ä½¿ç”¨ã€‚\"\"\"\n",
        "    docs = long_term_store.similarity_search(query, k=2)\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "# 1. å®šä¹‰è”ç½‘æœç´¢å·¥å…·\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"\n",
        "    å½“é—®é¢˜æ¶‰åŠé€šç”¨çŸ¥è¯†ï¼ˆå¦‚åäººã€æ—¶äº‹ã€ä½“è‚²ã€éæŠ€æœ¯ç±»é—®é¢˜ï¼‰æ—¶ï¼Œå¿…é¡»ä½¿ç”¨æ­¤å·¥å…·ã€‚\n",
        "    ä¸è¦ç”¨äº LangChain æˆ– RAG çš„æŠ€æœ¯å®šä¹‰ã€‚\n",
        "    \"\"\"\n",
        "    search = DuckDuckGoSearchRun()\n",
        "    return search.run(query)\n",
        "\n",
        "# 2. æ›´æ–°å·¥å…·åˆ—è¡¨ï¼ˆä¿ç•™åŸæœ‰çš„ï¼‰\n",
        "tools = [search_knowledge_base, search_long_term_memory, search_web]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Lz-RGiLkqx",
        "outputId": "4a52a039-de68-49b8-d249-1e71548cf0dc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ddgs\n",
            "  Downloading ddgs-9.10.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.1)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (1.0.0)\n",
            "Requirement already satisfied: lxml>=4.9.4 in /usr/local/lib/python3.12/dist-packages (from ddgs) (6.0.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Collecting fake-useragent>=2.2.0 (from ddgs)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.2.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
            "Downloading ddgs-9.10.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: socksio, fake-useragent, ddgs\n",
            "Successfully installed ddgs-9.10.0 fake-useragent-2.2.0 socksio-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "å®šä¹‰prompt"
      ],
      "metadata": {
        "id": "L8bEgO_3KJiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"\"\"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚\n",
        "\n",
        "å½“ç”¨æˆ·è¯¢é—® LangChainã€RAGã€Agent çš„æŠ€æœ¯ç»†èŠ‚æ—¶ï¼Œ\n",
        "ä¼˜å…ˆè°ƒç”¨ search_knowledge_base å·¥å…·ã€‚\n",
        "\n",
        "å½“ç”¨æˆ·è¯¢é—®å†å²æ€»ç»“æ—¶ï¼Œ\n",
        "è°ƒç”¨ search_long_term_memory å·¥å…·ã€‚\n",
        "\n",
        "å¦‚æœæ— éœ€æŸ¥è¯¢ï¼Œç›´æ¥å›ç­”ã€‚\n",
        "\"\"\"),\n",
        "    # ä¸å†ç”¨ input / chat_history\n",
        "])\n"
      ],
      "metadata": {
        "id": "2stQEomOLIyp"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç»‘å®šå·¥å…·åˆ° LLM"
      ],
      "metadata": {
        "id": "n0X2culzLqfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "C63sZnuxLtCc"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Planner èŠ‚ç‚¹"
      ],
      "metadata": {
        "id": "08IHwZmwLxdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# å¼•å…¥ DuckDuckGo\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "search_tool = DuckDuckGoSearchRun()\n",
        "\n",
        "def planner_node(state):\n",
        "    print(\"ğŸ”¥ è¿›å…¥ planner_node (åŠ¨æ€è§„åˆ’)\")\n",
        "    goal = state.get(\"goal\")\n",
        "\n",
        "    # æ ¸å¿ƒä¿®æ”¹ï¼šPrompt ä¸å†å¼ºåˆ¶å›ºå®šæ­¥éª¤ï¼Œè€Œæ˜¯è®© LLM é€‰æ‹©å·¥å…·\n",
        "    prompt = f\"\"\"\n",
        "    ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·ç›®æ ‡ç”Ÿæˆæ‰§è¡Œæ­¥éª¤åˆ—è¡¨ã€‚\n",
        "\n",
        "    å¯ç”¨åŠ¨ä½œ(Action)è¯´æ˜ï¼š\n",
        "    1. \"search_local\": ä»…å½“é—®é¢˜å…³äº LangChain, RAG, Agent ç­‰å†…éƒ¨æŠ€æœ¯æ–‡æ¡£æ—¶ä½¿ç”¨ã€‚\n",
        "    2. \"search_web\": å½“é—®é¢˜å…³äºåäºº(å¦‚ Steph Curry)ã€æ—¶äº‹ã€é€šç”¨çŸ¥è¯†æ—¶ä½¿ç”¨ã€‚\n",
        "    3. \"reason\": å½“é—®é¢˜æ˜¯é€»è¾‘æ¨ç†ã€æ•°å­¦è®¡ç®—ã€æˆ–æ™®é€šé—²èŠæ—¶ä½¿ç”¨ï¼ˆä¸éœ€è¦æœç´¢ï¼‰ã€‚\n",
        "    4. \"output\": æœ€åä¸€æ­¥å¿…é¡»æ˜¯ outputã€‚\n",
        "\n",
        "    ç”¨æˆ·ç›®æ ‡ï¼š\"{goal}\"\n",
        "\n",
        "    è¯·åªè¾“å‡ºä¸€ä¸ª JSON å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸è¦å…¶ä»–åºŸè¯ã€‚\n",
        "    ç¤ºä¾‹ 1 (æŠ€æœ¯é—®é¢˜): [\"search_local\", \"summarize\", \"output\"]\n",
        "    ç¤ºä¾‹ 2 (é€šç”¨é—®é¢˜): [\"search_web\", \"summarize\", \"output\"]\n",
        "    ç¤ºä¾‹ 3 (ç®€å•å¯¹è¯): [\"reason\", \"output\"]\n",
        "    \"\"\"\n",
        "\n",
        "    raw = run_llm(prompt)\n",
        "\n",
        "    import json, re\n",
        "    try:\n",
        "        plan = json.loads(raw)\n",
        "    except:\n",
        "        # æ­£åˆ™å…œåº•è§£æ\n",
        "        match = re.search(r\"(\\[.*\\])\", raw, re.S)\n",
        "        plan = json.loads(match.group(1)) if match else [\"reason\", \"output\"]\n",
        "\n",
        "    print(f\"âœ… åŠ¨æ€ç”Ÿæˆçš„è®¡åˆ’: {plan}\")\n",
        "\n",
        "    return {\n",
        "        \"plan\": plan,\n",
        "        \"current_step\": 0,\n",
        "        \"step_outputs\": [],\n",
        "        \"finished\": False,\n",
        "        \"critic\": {}\n",
        "    }"
      ],
      "metadata": {
        "id": "dFvOn-N-L0rN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executor èŠ‚ç‚¹"
      ],
      "metadata": {
        "id": "6l1PwQWYu9Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def executor_node(state):\n",
        "    plan = state.get(\"plan\")\n",
        "    idx = state.get(\"current_step\", 0)\n",
        "\n",
        "    if not plan or idx >= len(plan):\n",
        "        return {\"finished\": True}\n",
        "\n",
        "    step = plan[idx]\n",
        "    print(f\"âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: {step}\")\n",
        "\n",
        "    output = \"\"\n",
        "\n",
        "    # === åŠ¨ä½œåˆ†æ”¯ ===\n",
        "    if step == \"search_local\":\n",
        "        # æŸ¥æœ¬åœ°å‘é‡åº“\n",
        "        docs = retriever.invoke(state[\"goal\"])\n",
        "        output = f\"ã€æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘ï¼š\\n\" + \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    elif step == \"search_web\":\n",
        "        # æŸ¥ DuckDuckGo\n",
        "        print(\"   ğŸŒ æ­£åœ¨è”ç½‘æœç´¢...\")\n",
        "        try:\n",
        "            web_result = search_tool.invoke(state[\"goal\"])\n",
        "            output = f\"ã€äº’è”ç½‘æœç´¢ç»“æœã€‘ï¼š\\n{web_result}\"\n",
        "        except Exception as e:\n",
        "            output = f\"æœç´¢å¤±è´¥: {str(e)}\"\n",
        "\n",
        "    elif step == \"reason\":\n",
        "        # çº¯æ€è€ƒ/é—²èŠ\n",
        "        output = run_llm(f\"è¯·ç›´æ¥å›ç­”æˆ–æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼Œä¸è¦æœç´¢ï¼š{state['goal']}\")\n",
        "\n",
        "    elif step == \"summarize\":\n",
        "        # æ€»ç»“å‰é¢çš„æœç´¢ç»“æœ\n",
        "        context = \"\\n\\n\".join(state.get(\"step_outputs\", []))\n",
        "        output = run_llm(f\"åŸºäºä»¥ä¸‹æœç´¢åˆ°çš„èµ„æ–™ï¼Œå›ç­”ç”¨æˆ·ç›®æ ‡ï¼š'{state['goal']}'ã€‚\\n\\nèµ„æ–™ï¼š\\n{context}\")\n",
        "\n",
        "    elif step == \"output\":\n",
        "        # æœ€ç»ˆè¾“å‡ºæ•´ç†\n",
        "        prev_output = state[\"step_outputs\"][-1] if state[\"step_outputs\"] else \"\"\n",
        "        output = prev_output # ç›´æ¥é€ä¼ ä¸Šä¸€æ­¥çš„æ€»ç»“\n",
        "\n",
        "    else:\n",
        "        # å…œåº•\n",
        "        output = run_llm(f\"æ‰§è¡Œæ­¥éª¤ {step}ï¼Œå½“å‰ä¸Šä¸‹æ–‡ï¼š{state['step_outputs']}\")\n",
        "\n",
        "    return {\n",
        "        \"step_outputs\": state.get(\"step_outputs\", []) + [output],\n",
        "        \"current_step\": idx + 1\n",
        "    }"
      ],
      "metadata": {
        "id": "8MOU2M7KMBWB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Critic èŠ‚ç‚¹"
      ],
      "metadata": {
        "id": "0wM34XA1vIAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def critic_node(state):\n",
        "    import json, re # Ensure json is imported\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "ä½ æ˜¯å®¡ç¨¿äººã€‚\n",
        "\n",
        "ç›®æ ‡ï¼š\n",
        "{state[\"goal\"]}\n",
        "\n",
        "æ­¥éª¤ç»“æœï¼š\n",
        "{state[\"step_outputs\"]}\n",
        "\n",
        "åˆ¤æ–­ï¼š\n",
        "1. æ˜¯å¦å®Œæˆç›®æ ‡ï¼Ÿ\n",
        "2. æ˜¯å¦éœ€è¦é‡åšæŸä¸€æ­¥ï¼Ÿ\n",
        "\n",
        "è¾“å‡º JSONï¼š\n",
        "{{\n",
        "  \"accept\": true/false,\n",
        "  \"retry_step\": æ•°å­—æˆ–null\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    response = run_llm(prompt)\n",
        "\n",
        "    try:\n",
        "        verdict = json.loads(response)\n",
        "    except:\n",
        "        match = re.search(r\"(\\{{.*\\}})\", response, re.S)\n",
        "        if match:\n",
        "            verdict = json.loads(match.group(1))\n",
        "        else:\n",
        "            verdict = {\"accept\": False} # Default to fail if parse error\n",
        "\n",
        "    print(\"ğŸ§ Critic verdict:\", verdict)\n",
        "\n",
        "    updates = {\"critic\": verdict}\n",
        "\n",
        "    # Increment retries if not accepted\n",
        "    if not verdict.get(\"accept\"):\n",
        "        updates[\"retries\"] = state.get(\"retries\", 0) + 1\n",
        "\n",
        "    return updates"
      ],
      "metadata": {
        "id": "dozZSRdDvJOd"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ„å»º LangGraph"
      ],
      "metadata": {
        "id": "UaZLj0-XMFir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, List, Optional, Dict, Any\n",
        "\n",
        "# å®šä¹‰ State ç±»å‹ï¼Œä½¿ç”¨ TypedDict\n",
        "class AgentState(TypedDict, total=False):\n",
        "    goal: str\n",
        "    plan: Optional[List[str]]\n",
        "    current_step: int\n",
        "    step_outputs: List[str]\n",
        "    finished: bool\n",
        "    critic: Dict[str, Any]\n",
        "    retries: int\n",
        "\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"planner\", planner_node)\n",
        "graph.add_node(\"executor\", executor_node)\n",
        "graph.add_node(\"critic\", critic_node)\n",
        "\n",
        "graph.set_entry_point(\"planner\")\n",
        "\n",
        "graph.add_edge(\"planner\", \"executor\")\n",
        "\n",
        "def executor_decision(state):\n",
        "    if state.get(\"finished\"):\n",
        "        return \"critic\"\n",
        "    return \"executor\"\n",
        "\n",
        "\n",
        "graph.add_conditional_edges(\"executor\", executor_decision)\n",
        "\n",
        "def critic_decision(state):\n",
        "    verdict = state.get(\"critic\", {})\n",
        "    retries = state.get(\"retries\", 0)\n",
        "\n",
        "    if verdict.get(\"accept\"):\n",
        "        print(\"ğŸ‰ Critic accepted!\")\n",
        "        return END\n",
        "\n",
        "    if retries >= 3:\n",
        "        print(\"âš ï¸ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ\")\n",
        "        return END\n",
        "\n",
        "    print(f\"ğŸ”„ Critic rejected (Retries: {retries}). Re-planning...\")\n",
        "    return \"planner\"\n",
        "\n",
        "graph.add_conditional_edges(\"critic\", critic_decision)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "2MlPM2Q_MF0H"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æµ‹è¯•è¿è¡Œ"
      ],
      "metadata": {
        "id": "YIdKzgv_MKSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({\n",
        "    \"goal\": \"å†™ä¸€ç¯‡å…³äº RAG çš„ 300 å­—ä¸­æ–‡ç®€ä»‹\"\n",
        "}, config={\"recursion_limit\": 50})\n",
        "\n",
        "print(result[\"step_outputs\"][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuJeiLSLyD5Z",
        "outputId": "a48fd0c4-db0e-4182-aa32-84ed31d03071"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ è¿›å…¥ planner_node (åŠ¨æ€è§„åˆ’)\n",
            "âœ… åŠ¨æ€ç”Ÿæˆçš„è®¡åˆ’: ['search_local', 'summarize', 'output']\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: search_local\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: summarize\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: output\n",
            "ğŸ§ Critic verdict: {'accept': True, 'retry_step': None}\n",
            "ğŸ‰ Critic accepted!\n",
            "RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›ç­”é—®é¢˜æ—¶çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯æ—¶ï¼Œå¾€å¾€ä¾èµ–äºå…¶è®­ç»ƒæ—¶è·å¾—çš„çŸ¥è¯†ï¼Œå¯èƒ½ä¼šå¯¼è‡´å›ç­”çš„å‡†ç¡®æ€§ä¸è¶³ã€‚è€ŒRAGé€šè¿‡ç»“åˆä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå›ç­”æ—¶åŠ¨æ€åœ°ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œæå‡å›ç­”çš„è´¨é‡ã€‚\n",
            "\n",
            "åœ¨RAGçš„å·¥ä½œæµç¨‹ä¸­ï¼Œé¦–å…ˆä¼šæ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å‡ºç›¸å…³æ–‡æ¡£æˆ–ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯ä¸ç”¨æˆ·çš„è¾“å…¥ç»“åˆï¼Œç”Ÿæˆæ›´ä¸ºå‡†ç¡®å’Œä¸°å¯Œçš„å›ç­”ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å›ç­”çš„å‡†ç¡®æ€§ï¼Œè¿˜èƒ½ä½¿ç”Ÿæˆçš„å†…å®¹æ›´åŠ è´´åˆç”¨æˆ·çš„éœ€æ±‚ã€‚\n",
            "\n",
            "LangChainæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæ„å»ºLLMåº”ç”¨çš„æ¡†æ¶ï¼Œå®ƒä¸ºå¼€å‘è€…æä¾›äº†æ„å»ºå’Œé›†æˆRAGæŠ€æœ¯çš„å·¥å…·å’Œæ¥å£ã€‚é€šè¿‡LangChainï¼Œå¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°å®ç°ä¿¡æ¯æ£€ç´¢ä¸ç”Ÿæˆçš„ç»“åˆï¼Œåˆ›å»ºå‡ºæ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿå’Œåº”ç”¨ã€‚\n",
            "\n",
            "æ­¤å¤–ï¼ŒRAGè¿˜å¯ä»¥ä¸æ™ºèƒ½ä»£ç†ï¼ˆAgentï¼‰ç»“åˆï¼Œä»£ç†å¯ä»¥è°ƒç”¨å„ç§å·¥å…·æ¥æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºç³»ç»Ÿçš„åŠŸèƒ½å’Œçµæ´»æ€§ã€‚æ€»ä¹‹ï¼ŒRAGæŠ€æœ¯ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½¿å¾—æœºå™¨èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå›åº”äººç±»çš„éœ€æ±‚ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({\n",
        "    \"goal\": \"ä»‹ç»ä¸€ä¸‹Steph Curry\"\n",
        "}, config={\"recursion_limit\": 50})\n",
        "\n",
        "print(result[\"step_outputs\"][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss6ZOHAC3ltx",
        "outputId": "8e87f77e-d87d-425e-93c0-88de68d779f1"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ è¿›å…¥ planner_node (åŠ¨æ€è§„åˆ’)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:primp.impersonate:Impersonate 'edge_122' does not exist, using 'random'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… åŠ¨æ€ç”Ÿæˆçš„è®¡åˆ’: ['search_web', 'summarize', 'output']\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: search_web\n",
            "   ğŸŒ æ­£åœ¨è”ç½‘æœç´¢...\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: summarize\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: output\n",
            "ğŸ§ Critic verdict: {'accept': True, 'retry_step': None}\n",
            "ğŸ‰ Critic accepted!\n",
            "æ–¯è’‚èŠ¬Â·åº“é‡Œï¼ˆStephen Curryï¼‰æ˜¯ä¸€ä½ç¾å›½èŒä¸šç¯®çƒè¿åŠ¨å‘˜ï¼Œç°æ•ˆåŠ›äºNBAçš„é‡‘å·å‹‡å£«é˜Ÿã€‚ä»–è¢«å¹¿æ³›è®¤ä¸ºæ˜¯å†å²ä¸Šæœ€ä¼Ÿå¤§çš„å°„æ‰‹ä¹‹ä¸€ï¼Œä»¥å…¶å‡ºè‰²çš„ä¸‰åˆ†çƒèƒ½åŠ›è€Œé—»åã€‚åº“é‡Œäº1988å¹´3æœˆ14æ—¥å‡ºç”Ÿåœ¨ä¿„äº¥ä¿„å·çš„é˜¿å…‹ä¼¦ï¼Œåæ¥åœ¨åŒ—å¡ç½—æ¥çº³å·çš„æˆ´ç»´æ£®å­¦é™¢æ‰“çƒï¼Œå¹¶åœ¨2009å¹´NBAé€‰ç§€ä¸­è¢«é‡‘å·å‹‡å£«é˜Ÿä»¥ç¬¬ä¸ƒé¡ºä½é€‰ä¸­ã€‚\n",
            "\n",
            "åº“é‡Œåœ¨èŒä¸šç”Ÿæ¶¯ä¸­å–å¾—äº†è¯¸å¤šæˆå°±ï¼ŒåŒ…æ‹¬å¤šæ¬¡è·å¾—NBAæœ€æœ‰ä»·å€¼çƒå‘˜ï¼ˆMVPï¼‰å¥–é¡¹ï¼Œå¹¶å¸®åŠ©å‹‡å£«é˜Ÿèµ¢å¾—äº†å¤šä¸ªNBAæ€»å† å†›ã€‚ä»–çš„æ¯”èµ›é£æ ¼ä»¥å¿«é€Ÿçš„æŠ•ç¯®å’Œçµæ´»çš„è¿çƒè‘—ç§°ï¼Œæ”¹å˜äº†ç°ä»£ç¯®çƒçš„æ‰“æ³•ï¼Œæ¨åŠ¨äº†ä¸‰åˆ†çƒåœ¨æ¯”èµ›ä¸­çš„é‡è¦æ€§ã€‚\n",
            "\n",
            "é™¤äº†åœ¨çƒåœºä¸Šçš„æˆå°±ï¼Œåº“é‡Œè¿˜ç§¯æå‚ä¸æ…ˆå–„æ´»åŠ¨ï¼Œå¹¶åœ¨ç¤¾åŒºä¸­å‘æŒ¥å½±å“åŠ›ã€‚ä»–çš„èŒä¸šç”Ÿæ¶¯å’Œä¸ªäººé­…åŠ›ä½¿ä»–æˆä¸ºå…¨çƒç¯®çƒè¿·å¿ƒç›®ä¸­çš„å¶åƒã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({\n",
        "    \"goal\": \"ç»™æˆ‘è®²ä¸ªç¬‘è¯ã€‚\"\n",
        "}, config={\"recursion_limit\": 50})\n",
        "\n",
        "print(result[\"step_outputs\"][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5b-XnOG4jnk",
        "outputId": "8d0c335c-9d6a-41e0-e65d-8b48ae42f3f4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ è¿›å…¥ planner_node (åŠ¨æ€è§„åˆ’)\n",
            "âœ… åŠ¨æ€ç”Ÿæˆçš„è®¡åˆ’: ['reason', 'output']\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: reason\n",
            "âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: output\n",
            "ğŸ§ Critic verdict: {'accept': True, 'retry_step': None}\n",
            "ğŸ‰ Critic accepted!\n",
            "ä¸ºä»€ä¹ˆæ•°å­¦ä¹¦æ€»æ˜¯å¾ˆå¿§ä¼¤ï¼Ÿ\n",
            "\n",
            "å› ä¸ºå®ƒæœ‰å¤ªå¤šçš„é—®é¢˜ï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-Clym0u6fA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}