# -*- coding: utf-8 -*-
"""LangChainDemo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P12ywIZr4-BpcLxRFSMVDApERnYvNwAU
"""

!pip install -q langchain langchain-openai langchain-community faiss-cpu tiktoken

!pip uninstall -y langchain langchain-core langchain-community langgraph
!pip install -U langchain langchain-core langchain-community langgraph

from langchain_openai import ChatOpenAI
print("LangChain æ­£å¸¸")

import langchain_core
print(langchain_core.__version__)

import os
import getpass

os.environ["OPENAI_API_KEY"] = getpass.getpass("è¯·è¾“å…¥ä½ çš„ OpenAI API Key: ")

"""å‡†å¤‡ç®€æ˜“çŸ¥è¯†åº“"""

from langchain_core.documents import Document

docs = [
    Document(page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚"),
    Document(page_content="RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç”¨äºå¢å¼ºå¤§æ¨¡å‹å›ç­”çš„å‡†ç¡®æ€§ã€‚"),
    Document(page_content="Agent å¯ä»¥è°ƒç”¨å·¥å…·æ¥æ‰§è¡Œä»»åŠ¡ã€‚")
]

print("æ–‡æ¡£åˆ›å»ºæˆåŠŸ")

"""æ–‡æ¡£åˆ‡åˆ†"""

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)

split_docs = text_splitter.split_documents(docs)

print(f"åˆ‡åˆ†åçš„æ–‡æ¡£æ•°é‡: {len(split_docs)}")

"""å‘é‡åŒ– + å»ºç«‹å‘é‡æ•°æ®åº“"""

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

embeddings = OpenAIEmbeddings()

vectorstore = FAISS.from_documents(split_docs, embeddings)

retriever = vectorstore.as_retriever()

#embeddings å’Œé•¿æœŸå‘é‡åº“åˆå§‹åŒ–ï¼ˆè¿è¡Œä¸€æ¬¡ï¼‰
import time

long_term_store = FAISS.from_documents(
    [Document(page_content="åˆå§‹åŒ–")],
    embeddings
)

"""åˆ›å»º LLM"""

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)

def run_llm(prompt_text: str) -> str:
    result = llm.invoke(prompt_text)
    return result.content

"""å®šä¹‰ Tools"""

from langchain_core.tools import tool
!pip install -q duckduckgo-search
!pip install -U ddgs

# å·¥å…· 1ï¼šçŸ¥è¯†åº“æ£€ç´¢
@tool
def search_knowledge_base(query: str) -> str:
    """å½“ç”¨æˆ·è¯¢é—® LangChainã€RAG æˆ– Agent æŠ€æœ¯ç»†èŠ‚æ—¶ä½¿ç”¨ã€‚"""
    docs = retriever.invoke(query)
    return "\n\n".join([d.page_content for d in docs])

# å·¥å…· 2ï¼šé•¿æœŸè®°å¿†æ£€ç´¢
@tool
def search_long_term_memory(query: str) -> str:
    """å½“ç”¨æˆ·è¯¢é—®å†å²æ€»ç»“æˆ–ä¹‹å‰å¯¹è¯æ—¶ä½¿ç”¨ã€‚"""
    docs = long_term_store.similarity_search(query, k=2)
    return "\n\n".join([d.page_content for d in docs])

from langchain_community.tools import DuckDuckGoSearchRun

# 1. å®šä¹‰è”ç½‘æœç´¢å·¥å…·
@tool
def search_web(query: str) -> str:
    """
    å½“é—®é¢˜æ¶‰åŠé€šç”¨çŸ¥è¯†ï¼ˆå¦‚åäººã€æ—¶äº‹ã€ä½“è‚²ã€éæŠ€æœ¯ç±»é—®é¢˜ï¼‰æ—¶ï¼Œå¿…é¡»ä½¿ç”¨æ­¤å·¥å…·ã€‚
    ä¸è¦ç”¨äº LangChain æˆ– RAG çš„æŠ€æœ¯å®šä¹‰ã€‚
    """
    search = DuckDuckGoSearchRun()
    return search.run(query)

# 2. æ›´æ–°å·¥å…·åˆ—è¡¨ï¼ˆä¿ç•™åŸæœ‰çš„ï¼‰
tools = [search_knowledge_base, search_long_term_memory, search_web]

"""å®šä¹‰prompt"""

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

agent_prompt = ChatPromptTemplate.from_messages([
    ("system",
     """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚

å½“ç”¨æˆ·è¯¢é—® LangChainã€RAGã€Agent çš„æŠ€æœ¯ç»†èŠ‚æ—¶ï¼Œ
ä¼˜å…ˆè°ƒç”¨ search_knowledge_base å·¥å…·ã€‚

å½“ç”¨æˆ·è¯¢é—®å†å²æ€»ç»“æ—¶ï¼Œ
è°ƒç”¨ search_long_term_memory å·¥å…·ã€‚

å¦‚æœæ— éœ€æŸ¥è¯¢ï¼Œç›´æ¥å›ç­”ã€‚
"""),
    # ä¸å†ç”¨ input / chat_history
])

"""ç»‘å®šå·¥å…·åˆ° LLM"""

llm_with_tools = llm.bind_tools(tools)

"""Planner èŠ‚ç‚¹"""

# å¼•å…¥ DuckDuckGo
from langchain_community.tools import DuckDuckGoSearchRun
search_tool = DuckDuckGoSearchRun()

def planner_node(state):
    print("ğŸ”¥ è¿›å…¥ planner_node (åŠ¨æ€è§„åˆ’)")
    goal = state.get("goal")

    # æ ¸å¿ƒä¿®æ”¹ï¼šPrompt ä¸å†å¼ºåˆ¶å›ºå®šæ­¥éª¤ï¼Œè€Œæ˜¯è®© LLM é€‰æ‹©å·¥å…·
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·ç›®æ ‡ç”Ÿæˆæ‰§è¡Œæ­¥éª¤åˆ—è¡¨ã€‚

    å¯ç”¨åŠ¨ä½œ(Action)è¯´æ˜ï¼š
    1. "search_local": ä»…å½“é—®é¢˜å…³äº LangChain, RAG, Agent ç­‰å†…éƒ¨æŠ€æœ¯æ–‡æ¡£æ—¶ä½¿ç”¨ã€‚
    2. "search_web": å½“é—®é¢˜å…³äºåäºº(å¦‚ Steph Curry)ã€æ—¶äº‹ã€é€šç”¨çŸ¥è¯†æ—¶ä½¿ç”¨ã€‚
    3. "reason": å½“é—®é¢˜æ˜¯é€»è¾‘æ¨ç†ã€æ•°å­¦è®¡ç®—ã€æˆ–æ™®é€šé—²èŠæ—¶ä½¿ç”¨ï¼ˆä¸éœ€è¦æœç´¢ï¼‰ã€‚
    4. "output": æœ€åä¸€æ­¥å¿…é¡»æ˜¯ outputã€‚

    ç”¨æˆ·ç›®æ ‡ï¼š"{goal}"

    è¯·åªè¾“å‡ºä¸€ä¸ª JSON å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸è¦å…¶ä»–åºŸè¯ã€‚
    ç¤ºä¾‹ 1 (æŠ€æœ¯é—®é¢˜): ["search_local", "summarize", "output"]
    ç¤ºä¾‹ 2 (é€šç”¨é—®é¢˜): ["search_web", "summarize", "output"]
    ç¤ºä¾‹ 3 (ç®€å•å¯¹è¯): ["reason", "output"]
    """

    raw = run_llm(prompt)

    import json, re
    try:
        plan = json.loads(raw)
    except:
        # æ­£åˆ™å…œåº•è§£æ
        match = re.search(r"(\[.*\])", raw, re.S)
        plan = json.loads(match.group(1)) if match else ["reason", "output"]

    print(f"âœ… åŠ¨æ€ç”Ÿæˆçš„è®¡åˆ’: {plan}")

    return {
        "plan": plan,
        "current_step": 0,
        "step_outputs": [],
        "finished": False,
        "critic": {}
    }

"""Executor èŠ‚ç‚¹"""

def executor_node(state):
    plan = state.get("plan")
    idx = state.get("current_step", 0)

    if not plan or idx >= len(plan):
        return {"finished": True}

    step = plan[idx]
    print(f"âš™ï¸ Executor æ­£åœ¨æ‰§è¡Œ: {step}")

    output = ""

    # === åŠ¨ä½œåˆ†æ”¯ ===
    if step == "search_local":
        # æŸ¥æœ¬åœ°å‘é‡åº“
        docs = retriever.invoke(state["goal"])
        output = f"ã€æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘ï¼š\n" + "\n".join([d.page_content for d in docs])

    elif step == "search_web":
        # æŸ¥ DuckDuckGo
        print("   ğŸŒ æ­£åœ¨è”ç½‘æœç´¢...")
        try:
            web_result = search_tool.invoke(state["goal"])
            output = f"ã€äº’è”ç½‘æœç´¢ç»“æœã€‘ï¼š\n{web_result}"
        except Exception as e:
            output = f"æœç´¢å¤±è´¥: {str(e)}"

    elif step == "reason":
        # çº¯æ€è€ƒ/é—²èŠ
        output = run_llm(f"è¯·ç›´æ¥å›ç­”æˆ–æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼Œä¸è¦æœç´¢ï¼š{state['goal']}")

    elif step == "summarize":
        # æ€»ç»“å‰é¢çš„æœç´¢ç»“æœ
        context = "\n\n".join(state.get("step_outputs", []))
        output = run_llm(f"åŸºäºä»¥ä¸‹æœç´¢åˆ°çš„èµ„æ–™ï¼Œå›ç­”ç”¨æˆ·ç›®æ ‡ï¼š'{state['goal']}'ã€‚\n\nèµ„æ–™ï¼š\n{context}")

    elif step == "output":
        # æœ€ç»ˆè¾“å‡ºæ•´ç†
        prev_output = state["step_outputs"][-1] if state["step_outputs"] else ""
        output = prev_output # ç›´æ¥é€ä¼ ä¸Šä¸€æ­¥çš„æ€»ç»“

    else:
        # å…œåº•
        output = run_llm(f"æ‰§è¡Œæ­¥éª¤ {step}ï¼Œå½“å‰ä¸Šä¸‹æ–‡ï¼š{state['step_outputs']}")

    return {
        "step_outputs": state.get("step_outputs", []) + [output],
        "current_step": idx + 1
    }

"""Critic èŠ‚ç‚¹"""

def critic_node(state):
    import json, re # Ensure json is imported

    prompt = f"""
ä½ æ˜¯å®¡ç¨¿äººã€‚

ç›®æ ‡ï¼š
{state["goal"]}

æ­¥éª¤ç»“æœï¼š
{state["step_outputs"]}

åˆ¤æ–­ï¼š
1. æ˜¯å¦å®Œæˆç›®æ ‡ï¼Ÿ
2. æ˜¯å¦éœ€è¦é‡åšæŸä¸€æ­¥ï¼Ÿ

è¾“å‡º JSONï¼š
{{
  "accept": true/false,
  "retry_step": æ•°å­—æˆ–null
}}
"""

    response = run_llm(prompt)

    try:
        verdict = json.loads(response)
    except:
        match = re.search(r"(\{{.*\}})", response, re.S)
        if match:
            verdict = json.loads(match.group(1))
        else:
            verdict = {"accept": False} # Default to fail if parse error

    print("ğŸ§ Critic verdict:", verdict)

    updates = {"critic": verdict}

    # Increment retries if not accepted
    if not verdict.get("accept"):
        updates["retries"] = state.get("retries", 0) + 1

    return updates

"""æ„å»º LangGraph"""

from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Optional, Dict, Any

# å®šä¹‰ State ç±»å‹ï¼Œä½¿ç”¨ TypedDict
class AgentState(TypedDict, total=False):
    goal: str
    plan: Optional[List[str]]
    current_step: int
    step_outputs: List[str]
    finished: bool
    critic: Dict[str, Any]
    retries: int


graph = StateGraph(AgentState)

graph.add_node("planner", planner_node)
graph.add_node("executor", executor_node)
graph.add_node("critic", critic_node)

graph.set_entry_point("planner")

graph.add_edge("planner", "executor")

def executor_decision(state):
    if state.get("finished"):
        return "critic"
    return "executor"


graph.add_conditional_edges("executor", executor_decision)

def critic_decision(state):
    verdict = state.get("critic", {})
    retries = state.get("retries", 0)

    if verdict.get("accept"):
        print("ğŸ‰ Critic accepted!")
        return END

    if retries >= 3:
        print("âš ï¸ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ")
        return END

    print(f"ğŸ”„ Critic rejected (Retries: {retries}). Re-planning...")
    return "planner"

graph.add_conditional_edges("critic", critic_decision)

app = graph.compile()

"""æµ‹è¯•è¿è¡Œ"""

result = app.invoke({
    "goal": "å†™ä¸€ç¯‡å…³äº RAG çš„ 300 å­—ä¸­æ–‡ç®€ä»‹"
}, config={"recursion_limit": 50})

print(result["step_outputs"][-1])

result = app.invoke({
    "goal": "ä»‹ç»ä¸€ä¸‹Steph Curry"
}, config={"recursion_limit": 50})

print(result["step_outputs"][-1])

result = app.invoke({
    "goal": "ç»™æˆ‘è®²ä¸ªç¬‘è¯ã€‚"
}, config={"recursion_limit": 50})

print(result["step_outputs"][-1])

