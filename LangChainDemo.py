# -*- coding: utf-8 -*-
"""LangChainDemo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P12ywIZr4-BpcLxRFSMVDApERnYvNwAU
"""

!pip install -q langchain langchain-openai langchain-community faiss-cpu tiktoken

!pip uninstall -y langchain
!pip install langchain==0.1.20

from langchain_openai import ChatOpenAI
print("LangChain 正常")

import os
import getpass

os.environ["OPENAI_API_KEY"] = getpass.getpass("请输入你的 OpenAI API Key: ")

"""准备简易知识库"""

from langchain_core.documents import Document

docs = [
    Document(page_content="LangChain 是一个用于构建 LLM 应用的框架。"),
    Document(page_content="RAG 是检索增强生成技术，用于增强大模型回答的准确性。"),
    Document(page_content="Agent 可以调用工具来执行任务。")
]

print("文档创建成功")

"""文档切分"""

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)

split_docs = text_splitter.split_documents(docs)

print(f"切分后的文档数量: {len(split_docs)}")

"""向量化 + 建立向量数据库"""

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

embeddings = OpenAIEmbeddings()

vectorstore = FAISS.from_documents(split_docs, embeddings)

retriever = vectorstore.as_retriever()

#embeddings 和长期向量库初始化（运行一次）
import time

long_term_store = FAISS.from_documents(
    [Document(page_content="初始化")],
    embeddings
)

def store_summary_to_longterm(session_id, summary_text):
    doc = Document(
        page_content=summary_text,
        metadata={"session_id": session_id}
    )
    long_term_store.add_documents([doc])

"""创建 LLM"""

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)

"""创建 RAG Chain"""

from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

agent_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "你是一个智能助手。"
     "当用户询问 LangChain、RAG、Agent 的技术细节时，优先使用 search_knowledge_base 工具。"
     "当用户询问过去的对话总结或历史信息时，使用 search_long_term_memory 工具。"
     "如果问题是普通闲聊，不要调用工具。"),

    MessagesPlaceholder(variable_name="chat_history"),

    ("human", "{input}"),

    # 这个是必须的：Agent 思考过程
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])


# 工具 A：知识库检索
@tool
def search_knowledge_base(query: str) -> str:
    """当用户询问关于 LangChain、RAG 或 Agent 的具体定义和技术细节时，必须使用此工具。"""
    docs = retriever.invoke(query)
    return "\n\n".join([d.page_content for d in docs])

# 工具 B：长期记忆检索
@tool
def search_long_term_memory(query: str) -> str:
    """当用户询问之前的对话总结、或者很久以前的历史时，使用此工具查询长期记忆。"""
    docs = long_term_store.similarity_search(query, k=2)
    return "\n\n".join([d.page_content for d in docs])

tools = [search_knowledge_base, search_long_term_memory]

from langchain.agents import initialize_agent, AgentType
from langchain.agents import AgentExecutor

agent_executor = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,  # 这是关键
    verbose=True
)

"""创建 Planner"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

planner_prompt = ChatPromptTemplate.from_template("""
你是一个任务规划助手。

目标:
{goal}

请生成严格 4 个步骤：
1. 搜索资料（tool: search_knowledge_base）
2. 阅读资料（tool: search_knowledge_base）
3. 分析总结（tool: none）
4. 输出最终结果（tool: none）

输出 JSON 数组：

[
  {{
    "step_id": 1,
    "task": "...",
    "tool": "search_knowledge_base"
  }}
]
不要输出解释。
""")

planner = planner_prompt | llm | JsonOutputParser()

planner.invoke({"goal": "研究 RAG 的发展趋势"})

"""加入Critic

"""

# ===== Critic：检查每个步骤结果并给出 verdict（accept/minor_fix/major_fix）和建议 =====
from langchain_core.prompts import ChatPromptTemplate
import json

# 建议使用 cheaper LLM（小模型）来做 Critic，节省成本
critic_llm = llm  # 或者换成更cheap的 model，如 llm_small

critic_prompt = ChatPromptTemplate.from_template("""
你是一个审稿/评估助手。给定任务(step)的说明与执行结果，请评估该结果是否充分满足任务目标。

输入字段：
- step_id: {step_id}
- step_task: {step_task}
- step_output: {step_output}
- context: {context}   # 一段合并上下文（可由历史或检索提供）

请输出严格的 JSON，包含三个字段：
{{
  "verdict": "accept" | "minor_fix" | "major_fix",
  "score": float_between_0_and_1,
  "suggested_fix": "...（若无则空字符串）"
}}

说明：
- accept：结果充分，无需修改（score >= 0.8）
- minor_fix：需要小范围修改（score 0.5-0.8）
- major_fix：需要重做或重规划（score < 0.5）

只输出 JSON，不要多余文字。
""")

def run_critic(step_id: int, step_task: str, step_output: str, context: str = "") -> dict:
    """调用 LLM 做评审，返回 dict 格式 {verdict, score, suggested_fix}"""
    prompt_input = {
        "step_id": step_id,
        "step_task": step_task,
        "step_output": step_output,
        "context": context
    }
    # 把 prompt 和 llm 连起来
    critic_resp = critic_prompt | critic_llm
    raw = critic_resp.invoke(prompt_input)  # 这里与 llm 的接口有关，可能需要调整
    # raw 可能是字符串，尝试解析 JSON
    try:
        # 如果 invoke 返回复合结构，取 .content；否则直接用 raw
        text = raw.content if hasattr(raw, "content") else raw
        parsed = json.loads(text.strip())
    except Exception as e:
        # 解析失败时，做最保守的 fallback：判断为 major_fix 并返回 llm 的原始文本作为建议
        return {"verdict": "major_fix", "score": 0.0, "suggested_fix": text[:1000] if isinstance(text, str) else str(text)}
    return parsed

"""加入 Memory 包装"""

from langchain_community.chat_message_histories import ChatMessageHistory

memory_store = {}

def get_session_history(session_id: str):
    if session_id not in memory_store:
        memory_store[session_id] = ChatMessageHistory()
    return memory_store[session_id]

"""写Controller"""

def controller(inputs):
    goal = inputs["input"]
    session_id = inputs.get("session_id", "default")
    # 获取上下文或历史摘要，便于 critic 判断（可选）
    history = get_session_history(session_id).messages if hasattr(get_session_history(session_id), "messages") else None
    history_text = ""
    if history:
        # 把最近几条拼成字符串（或使用 summary）
        history_text = "\n".join([m.content if hasattr(m, "content") else str(m) for m in history[-6:]])

    # ① 先规划
    plan = planner.invoke({"goal": goal})

    final_answer_parts = []

    # ② 按步骤执行
    for step in plan:
        sid = step["step_id"]
        task = step["task"]
        tool_name = step.get("tool", "none")

        print(f"\n=== 执行步骤 {sid} ===")
        print("任务:", task, "tool:", tool_name)

        # 执行步骤（调用 agent 或直接 LLM）
        if tool_name != "none":
            exec_res = agent_executor.invoke({"input": task, "config": {"configurable": {"session_id": session_id}}})
            # exec_res 可能是 dict {'input':..., 'output':...}
            step_output = exec_res.get("output") if isinstance(exec_res, dict) else (exec_res.content if hasattr(exec_res, "content") else str(exec_res))
        else:
            llm_out = llm.invoke(task)
            step_output = llm_out.content if hasattr(llm_out, "content") else str(llm_out)

        print("步骤输出片段：", step_output[:300])

        # 调用 Critic
        critic_result = run_critic(sid, task, step_output, context=history_text)
        verdict = critic_result.get("verdict")
        score = critic_result.get("score")
        suggested = critic_result.get("suggested_fix", "")

        print("Critic verdict:", verdict, "score:", score)
        if verdict == "accept":
            final_answer_parts.append(step_output)
            # 可以把 critic 的评语也保存到短期 memory
            get_session_history(session_id).add_user_message(f"[critic] step {sid} accept score {score}")
        elif verdict == "minor_fix":
            # 用 LLM 进行小范围修正（patch）
            patch_prompt = f"请根据下面建议对结果做小范围修正。\n\n原结果：\n{step_output}\n\n建议：\n{suggested}\n\n请输出修正后的结果。"
            patched = llm.invoke(patch_prompt)
            patched_text = patched.content if hasattr(patched, "content") else str(patched)
            final_answer_parts.append(patched_text)
            # 存 Critic 记录
            get_session_history(session_id).add_user_message(f"[critic] step {sid} minor_fix score {score} suggestion: {suggested}")
        else:  # major_fix
            # 重大问题：触发 RePlan（简单重询 Planner，并把失败信息传进去）
            replan_prompt_goal = f"上次执行 step {sid} 的任务是：{task}，输出是：{step_output}。Critic 认为需要 major_fix：{suggested}。请基于原始目标重新生成4步计划，并优先避免上一步的问题。"
            new_plan = planner.invoke({"goal": replan_prompt_goal})
            # 选择替换当前 plan（简单策略：用 new_plan 替换后续步骤并继续执行）
            remaining_index = None
            # 找当前 step 在 plan 中的位置并替换
            for i, p in enumerate(plan):
                if p["step_id"] == sid:
                    remaining_index = i
                    break
            if remaining_index is not None:
                plan = plan[:remaining_index] + new_plan  # 用新计划替换当前位置及后续
            # 记录 critic
            get_session_history(session_id).add_user_message(f"[critic] step {sid} major_fix score {score} suggestion: {suggested}")
            # 不把当前 step 输出纳入 final_answer（因为无效），直接继续循环执行新计划
            continue

    # 聚合最终回答
    final_answer = "\n\n".join(final_answer_parts)
    return {"answer": final_answer}

"""用 RunnableWithMessageHistory 包装 agent_executor"""

from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory

controller_runnable = RunnableLambda(controller)

agent_system = RunnableWithMessageHistory(
    controller_runnable,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

session_id = "user1"

print(agent_system.invoke(
    {"input": "什么是 RAG？"},
    config={"configurable": {"session_id": session_id}}
))

session_id = "user1"

print(agent_system.invoke(
    {"input": "Steph Curry是谁？"},
    config={"configurable": {"session_id": session_id}}
))

session_id = "user2"

print(agent_system.invoke(
    {"input": "它和Lebron有什么关系？"},
    config={"configurable": {"session_id": session_id}}
))

session_id = "user1"

print(agent_system.invoke(
    {"input": "什么是RAG？不要用知识库里的内容"},
    config={"configurable": {"session_id": session_id}}
))

